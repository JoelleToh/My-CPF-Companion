{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5478b-9096-46f0-a62b-caa3a3bca165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "if load_dotenv('.env'):\n",
    "# for local development\n",
    "    OPENAI_KEY=os.getenv('OPENAI_API_KEY')\n",
    "else:\n",
    "    OPENAI_KEY=st.secrets['OPENAI_API_KEY']\n",
    "\n",
    "# Pass the API Key to the OpenAI Client\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "# Helper function for getting embedding\n",
    "def get_embedding(input, model='text-embedding-3-small'):\n",
    "    response = client.embeddings.create(\n",
    "        input=input,\n",
    "        model=model\n",
    "    )\n",
    "    return [x.embedding for x in response.data]\n",
    "\n",
    "# Note that this function directly take in \"messages\" as the parameter.\n",
    "def get_completion(messages, model=\"gpt-4o-mini\", temperature=0, top_p=1.0, max_tokens=1024, n=1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        n=1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689dc89-bbf2-462c-8a59-baaaf65e7062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from helper_functions.utility import check_password\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# region <--------- Streamlit App Configuration ---------> \n",
    "st.set_page_config(\n",
    "    layout=\"centered\",\n",
    "    page_title=\"CPF Policy Assistant\"\n",
    ")\n",
    "# Check if the password is correct.\n",
    "if not check_password():\n",
    "    st.stop()\n",
    "\n",
    "# endregion <--------- Streamlit App Configuration --------->\n",
    "\n",
    "st.title(\"ðŸ“š My CPF Policy Assistant\")\n",
    "\n",
    "### Provide the Bot with CPF news from Oct 23 since GPT-4o mini has knowledge cutoff of October 2023\n",
    "# Get the name of the files in the folder\n",
    "dir_path = r'./News Releases'\n",
    "filename_list = []\n",
    "\n",
    "# Iterate directory\n",
    "for path in os.listdir(dir_path):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "        filename_list.append(path)\n",
    "\n",
    "# Load PDF documents\n",
    "list_of_documents_loaded = []\n",
    "\n",
    "for filename in filename_list:\n",
    "    if filename.endswith('.pdf'): \n",
    "        try:\n",
    "            # Load the document\n",
    "            pdf_path = os.path.join(dir_path, filename)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            data = loader.load()\n",
    "            \n",
    "            # Merge data into a single string if necessary\n",
    "            if isinstance(data, list):\n",
    "                full_text = ' '.join([page.page_content for page in data])\n",
    "            else:\n",
    "                full_text = data\n",
    "            \n",
    "            list_of_documents_loaded.append(Document(page_content=full_text))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Create embeddings and vector database\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=list_of_documents_loaded,\n",
    "    embedding=embeddings_model,\n",
    "    collection_name=\"naive_splitter\", \n",
    "    persist_directory=\"./vector_db\"\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "llm_model = ChatOpenAI(model='gpt-4o-mini', temperature=0, seed=42)\n",
    "rag_chain = RetrievalQA.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm_model\n",
    ")\n",
    "\n",
    "form = st.form(key=\"form\")\n",
    "form.subheader(\"CPF Policy Explainer\")\n",
    "user_prompt2 = form.text_area(\"Enter your query regarding CPF Policy here\", height=200)\n",
    "\n",
    "# Initialize session state if not already done\n",
    "if 'LLM_reply' not in st.session_state:\n",
    "    st.session_state['LLM_reply'] = ''\n",
    "\n",
    "if 'user_prompt1' not in st.session_state:\n",
    "    st.session_state['user_prompt1'] = ''\n",
    "\n",
    "# Form submission\n",
    "if form.form_submit_button(\"Submit\"):\n",
    "    # Prepare the system message\n",
    "    system_message = \"\"\"You are a helpful assistant from the Central Provident Fund (CPF) of Singapore,\n",
    "    you are well-versed in CPF Policy. \n",
    "\n",
    "    Understand the customer service query and decide if the query is related to CPF policy.\n",
    "    If the query is related to CPF policy, proceed to reply using information from https://www.cpf.gov.sg/ and ensure it is based on the Singapore context.\n",
    "    If the query is NOT related to CPF policy, reply: I'm unable to assist as the enquiry is not related to CPF policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare the conversation history\n",
    "    conversation_history = \"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": st.session_state['user_prompt1']},\n",
    "        {\"role\": \"assistant\", \"content\": st.session_state['LLM_reply']},\n",
    "        {\"role\": \"user\", \"content\": user_prompt2}\n",
    "    ]\n",
    "\n",
    "    # Concatenate the conversation history into a single string\n",
    "    for message in messages:\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "        conversation_history += f\"{role.capitalize()}: {content}\\n\"\n",
    "\n",
    "    # Prepare the full input (conversation history + latest user query)\n",
    "    full_input = f\"Conversation history:\\n{conversation_history}\\nUser query: {user_prompt2}\"\n",
    "\n",
    "    # Get the response from the RAG chain\n",
    "    response = rag_chain.run(full_input)\n",
    "\n",
    "    # Update session state with the new user input and LLM response\n",
    "    st.session_state['user_prompt1'] = user_prompt2\n",
    "    st.session_state['LLM_reply'] = response\n",
    "\n",
    "    # This displays the response generated by the LLM onto the frontend\n",
    "    st.write(response)\n",
    "    print(f\"User Input is {user_prompt2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
